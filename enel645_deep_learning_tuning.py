# -*- coding: utf-8 -*-
"""enel645_deep_learning_tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16u8AdnMk0rImn6HaG3RJve2MoY249Y-K
"""

# Install Keras Tuner
!pip install keras-tuner --upgrade

!nvidia-smi

import pickle
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from keras_tuner.tuners import Hyperband
from keras_tuner.tuners import RandomSearch
from sklearn.metrics import accuracy_score, f1_score
import os

# Load train_dfs_dict
with open('train_dfs_dict.pkl', 'rb') as f:
    train_dfs_dict = pickle.load(f)

# Load test_dfs_dict
with open('test_dfs_dict.pkl', 'rb') as f:
    test_dfs_dict = pickle.load(f)

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# dataset identified as performing best across models
train_key = "train_hsa-mir-7-5p"
test_key = "test_hsa-mir-7-5p"

X_train = train_dfs_dict[train_key].iloc[:, :-1]
y_train = train_dfs_dict[train_key]['LABEL'].astype(int)

X_test = test_dfs_dict[test_key].iloc[:, :-1]
y_test = test_dfs_dict[test_key]['LABEL'].astype(int)

# Function to create the model (required for Keras Tuner)
def build_model(hp):
    model = Sequential()
    model.add(Dense(units=hp.Int('units_1', min_value=32, max_value=512, step=32),
                    activation='relu', input_shape=(X_train.shape[1],)))
    model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.0, max_value=0.5, step=0.1)))
    model.add(Dense(units=hp.Int('units_2', min_value=32, max_value=512, step=32),
                    activation='relu'))
    model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.0, max_value=0.5, step=0.1)))
    model.add(Dense(1, activation='sigmoid'))

    # Compile the model
    model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model

# Initialize the tuner
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,  # Set this to a reasonable number to limit search time
    executions_per_trial=1,
    directory='my_dir',
    project_name='keras_tuning'
)

# Perform hyperparameter tuning
tuner.search(X_train, y_train, epochs=20, validation_split=0.2)

# Get the best model
best_model = tuner.get_best_models(num_models=1)[0]

# Predictions on the test set
y_pred = best_model.predict(X_test)
y_pred = np.round(y_pred).astype(int)  # Converting probabilities to binary output

# Evaluate the model on the test set
test_loss, test_acc = best_model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")

# Calculate and print the F1 score
f1 = f1_score(y_test, y_pred)
print(f"Test F1 Score: {f1:.4f}")